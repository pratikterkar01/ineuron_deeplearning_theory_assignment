1.What are the advantages of a CNN over a fully connected DNN for image classification?
Ans:-CNNs are specifically designed for image data and take advantage of the spatial structure and hierarchical features present in images. 
These characteristics make CNNs superior to fully connected DNNs for tasks like image classification.
a.Reduced parameter count:-CNNs typically have far fewer parameters compared to fully connected DNNs of similar depth.
b.Translation invariance:- This means that they can recognize objects in different positions within an image without needing to relearn features for each position.
In contrast, fully connected DNNs treat every input neuron as independent, making them sensitive to the exact position of features.
c.Weight sharing and feature reuse: 

2.Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of
2, and "same" padding. The lowest layer outputs 100 feature maps, the middle one outputs
200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.
What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much
RAM will this network require when making a prediction for a single instance? What about when
training on a mini-batch of 50 images?

Ans:-
Total number of parameters:-
1st layer:-3*(kernel_size)*

3.. If your GPU runs out of memory while training a CNN, what are five things you could try to
solve the problem?

Ans:-
1.Reduce the Batch Size:-
2.Decress the Model COmplexity
3.Use Mixed Precision Traning
4.Gradient Checkpoint
5.Data Augmentation
6.Reduce the image size

4.Why would you want to add a max pooling layer rather than a convolutional layer with the
same stride?
Ans:-Max pooling layers and convolutional layers with the same stride both downsample the spatial dimensions of feature maps,
but they serve different purposes in a Convolutional Neural Network (CNN). 
Choosing between them depends on the specific objectives of your network and the features you want to capture.
Here are reasons why you might want to add a max pooling layer instead of a convolutional layer with the same stride:
1.Dimensionality Reduction:
2.Translation Invariance
3.Feature Selection
4.Reducing Overfitting:

5.When would you want to add a local response normalization layer?
Ans:-Local Response Normalization (LRN) layers were once a popular component in Convolutional Neural Networks (CNNs),
especially in earlier architectures like AlexNet. However, they are less commonly used today due to the emergence of other normalization techniques 
like Batch Normalization and Layer Normalization, which have proven to be more effective in many cases.
Nonetheless, there are situations where you might consider adding an LRN layer

6.Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main
innovations in GoogLeNet, ResNet, SENet, and Xception?
Ans:-
1.Replicating Older Architectures
2.Model Interpretability
3.Specific Architecture Consideration
4.Ensemble Models

main inovations in GoogleNet,Reset,Senet, Xception
1.ResNet:
Deep architecture
Residual Blocks
2.GoogleNet
Inception Module
Global avg pooling
multiple branches




7.What is a fully convolutional network? How can you convert a dense layer into a
convolutional layer?
Ans:-fuly connected cnn means the image 1st  passed to the cnn layer and get the output as convoluted image
this image is in the form of the picxel so we convert into the 1-d array means making it the flat
it can make the cnn image to the flatten by using flayyen()

8.What is the main technical difficulty of semantic segmentation?
Ans:-
1.pixel wise classification
2.object occlusiion
3.object Scale Variation
4.Fine-grained object Boundries
5.Complex scences
6.Limited data
7.Generalization

9. Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.
Ans:=95%

10.Use transfer learning for large image classification, going through these steps:
.
